{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
    "\n",
    "torch.manual_seed(305)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "SMALL_ITERS = 1000\n",
    "LARGE_ITERS = 2000\n",
    "EVAL_ITERS = 100\n",
    "CONTEXT_WINDOW_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 4,573,338\n"
     ]
    }
   ],
   "source": [
    "input_file_path = 'data/full_shakespeare.txt'\n",
    "\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "train has 1,478,711 tokens\n",
      "val has 163,429 tokens\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 2000\n",
    "\n",
    "# Define a BPE model\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Trainer with desired vocab size\n",
    "trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<unk>\"])\n",
    "# data should be an iterator over your text lines or documents\n",
    "tokenizer.train_from_iterator([data], trainer=trainer)\n",
    "\n",
    "# Encode and decode functions\n",
    "def encode_bpe(text):\n",
    "    return tokenizer.encode(text).ids\n",
    "\n",
    "def decode_bpe(ids):\n",
    "    return tokenizer.decode(ids)\n",
    "\n",
    "# Example usage:\n",
    "train_text = data[:int(len(data) * 0.9)]\n",
    "val_text = data[int(len(data) * 0.9):]\n",
    "\n",
    "train_tokens = encode_bpe(train_text)\n",
    "val_tokens = encode_bpe(val_text)\n",
    "\n",
    "import torch\n",
    "train_data = torch.tensor(train_tokens)\n",
    "val_data = torch.tensor(val_tokens)\n",
    "\n",
    "print(f\"train has {len(train_data):,} tokens\")\n",
    "print(f\"val has {len(val_data):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting batches of data\n",
    "def get_batch(split, context_window_size, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    generate a small batch of data of inputs x and targets y\n",
    "\n",
    "    Args:\n",
    "        split: 'train' or 'val'\n",
    "        device: 'cpu' or 'cuda' (should be 'cuda' if available)\n",
    "    \"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_window_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_window_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_window_size+1] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# helper function for tracking loss during training\n",
    "# given to you\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters, context_window_size, device, use_focal_loss=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      model: model being evaluated\n",
    "      eval_iters: number of batches to average over\n",
    "      context_window_size: size of the context window\n",
    "      device: 'cpu' or 'cuda' (should be 'cuda' if available)\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses_by_type = {\n",
    "            'ce_loss': torch.zeros(eval_iters),\n",
    "        }\n",
    "        if use_focal_loss:\n",
    "            losses_by_type['f_loss'] = torch.zeros(eval_iters)\n",
    "        \n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, context_window_size, device)\n",
    "            logits, ce_loss, f_loss = model(X, Y)\n",
    "            losses_by_type['ce_loss'][k] = ce_loss.item()\n",
    "            if use_focal_loss:\n",
    "                losses_by_type['f_loss'][k] = f_loss.item()\n",
    "\n",
    "        out[split] = {'ce_loss': losses_by_type['ce_loss'].mean().item()}\n",
    "        if use_focal_loss:\n",
    "            out[split]['f_loss'] = losses_by_type['f_loss'].mean().item()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, context_window_size, embed_size=384):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          head_size: int, size of the head embedding dimension (K)\n",
    "          context_window_size: int, number of tokens considered in the past for attention (T)\n",
    "          embed_size: int, size of the token embedding dimension (D)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "        # not a param of the model, so registered as a buffer\n",
    "        self.register_buffer('tril', torch.tril(\n",
    "            torch.ones(context_window_size, context_window_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: (B,T,D) tensor of token embeddings\n",
    "\n",
    "        Returns:\n",
    "          (B,T,D) tensor of attention-weighted token embeddings\n",
    "        \"\"\"\n",
    "        # TODO: your code here\n",
    "        B, T, _ = x.shape\n",
    "        K = self.head_size\n",
    "        key = self.key(x)\n",
    "        query = self.query(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        attn_scores = query@key.transpose(-2, -1)\n",
    "        causal_mask = self.tril[:T, :T][None, :, :]\n",
    "        attn_scores = attn_scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "        attn_weights = torch.softmax(attn_scores / (K ** 0.5), dim=-1)\n",
    "        return attn_weights@value\n",
    "\n",
    "class SingleHeadedAttentionLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, context_window_size, head_size, embed_size=384):\n",
    "      \"\"\"\n",
    "      Args:\n",
    "        vocab_size: int, size of the vocabulary (V)\n",
    "        context_window_size: int, number of tokens considered in the past for attention (T)\n",
    "        head_size: int, size of the head embedding dimension (K)\n",
    "        embed_size: int, size of the token embedding dimension (D)\n",
    "      \"\"\"\n",
    "      super().__init__()\n",
    "      self.vocab_size = vocab_size\n",
    "      self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
    "      self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
    "      self.context_window_size = context_window_size\n",
    "\n",
    "      # TODO: your code below\n",
    "      self.atten_head = Head(head_size, context_window_size, embed_size)\n",
    "      self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, token_ids, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry\n",
    "                     in the batch has length T)\n",
    "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
    "\n",
    "        Returns:\n",
    "          logits: (B, T, V) logits[b,t] gives the length V vector of logits for the next token\n",
    "                   prediction in string b up to t tokens\n",
    "          loss: scalar, negative log likelihood of target given context\n",
    "        \"\"\"\n",
    "        B, T = token_ids.shape # (batch size, length)\n",
    "        tok_emb = self.token_embedding_table(token_ids) # (B,T,D)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,D)\n",
    "        x = tok_emb + pos_emb # (B,T,D)\n",
    "        x = self.atten_head(x) # (B,T,D)\n",
    "        logits = self.lm_head(x) # (B,T,V)\n",
    "\n",
    "        # TODO: your code here\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, token_ids, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          token_ids: (B, T) tensor of token ids to provide as context\n",
    "          max_new_tokens: int, maximum number of new tokens to generate\n",
    "\n",
    "        Returns:\n",
    "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "        # your code below\n",
    "        B, T = token_ids.shape\n",
    "        new_token_ids = token_ids.clone()\n",
    "        for t in range(max_new_tokens):\n",
    "            logits = self(new_token_ids)\n",
    "            new_token = torch.multinomial(F.softmax(logits[:, -1, :], dim=-1), 1)\n",
    "            new_token_ids = torch.cat([new_token_ids, new_token], dim=1)\n",
    "        return new_token_ids\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, context_window_size, num_heads, head_size, embed_size=384):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context_window_size: int, number of tokens considered in the past for attention (T)\n",
    "            num_heads: int, number of heads (H)\n",
    "            head_size: int, size of the head embedding dimension\n",
    "            embed_size: int, size of the token embedding dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO, your code below\n",
    "        self.heads = nn.ModuleList([Head(head_size, context_window_size, embed_size) for _ in range(num_heads)])\n",
    "        self.lm_head = nn.Linear(embed_size*num_heads, embed_size)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO, your code below\n",
    "        B, T, _ = x.shape\n",
    "        head_size = x.shape[-1] // self.num_heads\n",
    "        head_outputs = [head(x) for head in self.heads]\n",
    "        head_outputs = torch.cat(head_outputs, dim=-1)\n",
    "        head_outputs = head_outputs.view(B, T, -1)\n",
    "        return self.lm_head(head_outputs)\n",
    "\n",
    "class MultiHeadedAttentionLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6):\n",
    "      super().__init__()\n",
    "      self.head_size = embed_size // num_heads\n",
    "      self.context_window_size = context_window_size\n",
    "      # TODO: your code below\n",
    "      self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
    "      self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
    "      self.multi_head_attention = MultiHeadAttention(context_window_size, num_heads, self.head_size, embed_size)\n",
    "      self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "      self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, token_ids, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry in the\n",
    "                     batch has length T)\n",
    "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
    "\n",
    "        Returns:\n",
    "          logits: (B, T, V), logits[b,t] gives the length V vector of logits for the next token\n",
    "                  prediction in string b up to t tokens\n",
    "          loss: scalar, negative log likelihood of target given context\n",
    "        \"\"\"\n",
    "        # TODO: your code below\n",
    "        loss = None\n",
    "        B, T = token_ids.shape\n",
    "        tok_emb = self.token_embedding_table(token_ids)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.multi_head_attention(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, token_ids, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          token_ids: (B, T) tensor of token ids to provide as context\n",
    "          max_new_tokens: int, maximum number of new tokens to generate\n",
    "\n",
    "        Returns:\n",
    "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
    "        \"\"\"\n",
    "        # TODO: your code below\n",
    "        for t in range(max_new_tokens):\n",
    "            if token_ids.shape[1] > self.context_window_size:\n",
    "                token_ids = token_ids[:, -self.context_window_size:]\n",
    "            B, T = token_ids.shape\n",
    "            logits, loss = self.forward(token_ids)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            new_token = torch.multinomial(probs, 1)\n",
    "            token_ids = torch.cat([token_ids, new_token], dim=1)\n",
    "        return token_ids\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity\n",
    "        Given to you, you don't need to write any code here!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\" Transformer block: communication across sequence length, followed by communication across embedding space\n",
    "        Uses multi-headed attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # TODO: your code below\n",
    "        self.feed_forward = FeedForward(embed_size)\n",
    "        self.atten_heads = MultiHeadAttention(context_window_size, num_heads, embed_size // num_heads, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.atten_heads(self.ln1(x)) # communication over sequence length\n",
    "        x = x + self.feed_forward(self.ln2(x)) # communication across embedding space\n",
    "        return x\n",
    "\n",
    "def focal_loss(logits, targets, gamma=2.0, alpha=1.0, reduction='mean'):\n",
    "    ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    focal_factor = (1 - pt) ** gamma\n",
    "    loss = alpha * focal_factor * ce_loss\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "class TransformerLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6, n_layers=6):\n",
    "        \"\"\"\n",
    "          Args:\n",
    "              vocab_size: int, number of tokens in the vocabulary (V)\n",
    "              context_window_size: int, size of the context window (T)\n",
    "              embed_size: int, embedding size (D)\n",
    "              num_heads: int, number of heads (H)\n",
    "              n_layers: int, number of layers (M)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(vocab_size,\n",
    "                             context_window_size,\n",
    "                             embed_size=embed_size,\n",
    "                             num_heads=num_heads)\n",
    "            for _ in range(n_layers)])\n",
    "\n",
    "        # final layer norm\n",
    "        self.ln_f = nn.LayerNorm(embed_size)\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "        # good initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, token_ids, targets=None):\n",
    "        \"\"\"\n",
    "        Agrgs:\n",
    "            token_ids: tensor of integers, provides the contet, shape (B, T)\n",
    "            targets: tensor of integers, provides the tokens we are preidcitng, shape (B, T)\n",
    "        \"\"\"\n",
    "        B, T = token_ids.shape\n",
    "\n",
    "        # token_ids and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(token_ids) # (B, T, D)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, D)\n",
    "        x = tok_emb + pos_emb # (B, T, D)\n",
    "\n",
    "        # TODO: your code below\n",
    "        loss = None\n",
    "        logits = self.blocks(x)\n",
    "        logits = self.ln_f(logits)\n",
    "        logits = self.lm_head(logits)\n",
    "        if targets is not None:\n",
    "            ce_loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
    "            f_loss = focal_loss(logits.view(-1, logits.shape[-1]), targets.view(-1), gamma=0.5, alpha=1.0)\n",
    "        return logits, ce_loss, f_loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, token_ids, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: tensor of integers forming the context, shape (B, T)\n",
    "            max_new_tokens: int, max number of tokens to generate\n",
    "        \"\"\"\n",
    "        # TOOD, your code below\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            if token_ids.size(1) > CONTEXT_WINDOW_SIZE:\n",
    "                token_ids = token_ids[:, -CONTEXT_WINDOW_SIZE:]\n",
    "            logits, _, _ = self(token_ids)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "        self.train()\n",
    "        return token_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = TransformerLM(vocab_size, CONTEXT_WINDOW_SIZE)\n",
    "tlm = trans.to(device)\n",
    "learning_rate = 1e-4\n",
    "# TODO, your code below\n",
    "\n",
    "optimizer = optim.Adam(tlm.parameters(), lr=learning_rate)\n",
    "\n",
    "eval_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2000 [00:06<3:41:50,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss {'ce_loss': 7.698077201843262, 'f_loss': 7.6962103843688965}, val loss {'ce_loss': 7.696474552154541, 'f_loss': 7.694606781005859}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 201/2000 [00:49<1:03:53,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200: train loss {'ce_loss': 5.164602756500244, 'f_loss': 5.128971099853516}, val loss {'ce_loss': 5.232719898223877, 'f_loss': 5.1980133056640625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 401/2000 [01:31<56:03,  2.10s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400: train loss {'ce_loss': 4.694948196411133, 'f_loss': 4.65223503112793}, val loss {'ce_loss': 4.802484035491943, 'f_loss': 4.7613019943237305}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 601/2000 [02:13<49:01,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 600: train loss {'ce_loss': 4.389745712280273, 'f_loss': 4.34204626083374}, val loss {'ce_loss': 4.550459384918213, 'f_loss': 4.50554084777832}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 801/2000 [02:56<42:00,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 800: train loss {'ce_loss': 4.173600673675537, 'f_loss': 4.1227874755859375}, val loss {'ce_loss': 4.386159896850586, 'f_loss': 4.3393754959106445}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1001/2000 [03:38<35:00,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss {'ce_loss': 3.976705551147461, 'f_loss': 3.919642925262451}, val loss {'ce_loss': 4.241973876953125, 'f_loss': 4.190536975860596}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1201/2000 [04:20<28:00,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1200: train loss {'ce_loss': 3.8132996559143066, 'f_loss': 3.7534191608428955}, val loss {'ce_loss': 4.129662990570068, 'f_loss': 4.076267242431641}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1401/2000 [05:03<20:59,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1400: train loss {'ce_loss': 3.6882379055023193, 'f_loss': 3.6249749660491943}, val loss {'ce_loss': 4.036251544952393, 'f_loss': 3.9799365997314453}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1601/2000 [05:45<13:58,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1600: train loss {'ce_loss': 3.575105667114258, 'f_loss': 3.5091559886932373}, val loss {'ce_loss': 3.9810876846313477, 'f_loss': 3.922952175140381}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1801/2000 [06:27<06:58,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1800: train loss {'ce_loss': 3.491826057434082, 'f_loss': 3.4230458736419678}, val loss {'ce_loss': 3.9159350395202637, 'f_loss': 3.8552422523498535}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:03<00:00,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final training loss = 3.664591073989868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f_loss_list = []\n",
    "ce_loss_list = []\n",
    "for it in tqdm(range(LARGE_ITERS)):\n",
    "    # Evaluate\n",
    "    if it % eval_interval == 0:\n",
    "        losses = estimate_loss(tlm, EVAL_ITERS, CONTEXT_WINDOW_SIZE, device, use_focal_loss=True)\n",
    "        print(f\"step {it}: train loss {losses['train']}, val loss {losses['val']}\")\n",
    "    \n",
    "    # Forward/backward/update\n",
    "    xb, yb = get_batch('train', CONTEXT_WINDOW_SIZE, device, batch_size = 64)\n",
    "    logits, ce_loss, f_loss = tlm(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    ce_loss.backward()\n",
    "    optimizer.step()\n",
    "    f_loss_list.append(f_loss.item())\n",
    "    ce_loss_list.append(ce_loss.item())\n",
    "\n",
    "print(\"final CE training loss =\", ce_loss_list[-1])\n",
    "print(\"final F training loss =\", f_loss_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'ce_loss': 3.3895068168640137, 'f_loss': 3.319181203842163},\n",
       " 'val': {'ce_loss': 3.8796470165252686, 'f_loss': 3.817950487136841}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss(tlm, EVAL_ITERS, CONTEXT_WINDOW_SIZE, device, use_focal_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2000 [00:06<3:44:11,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss {'ce_loss': 6.214792251586914, 'f_loss': 6.194438934326172}, val loss {'ce_loss': 6.249657154083252, 'f_loss': 6.229870796203613}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 201/2000 [00:49<1:03:02,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200: train loss {'ce_loss': 5.032546520233154, 'f_loss': 4.993616104125977}, val loss {'ce_loss': 5.114448070526123, 'f_loss': 5.076656341552734}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 401/2000 [01:31<56:35,  2.12s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400: train loss {'ce_loss': 4.642302989959717, 'f_loss': 4.595801830291748}, val loss {'ce_loss': 4.772821426391602, 'f_loss': 4.728541374206543}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 601/2000 [02:13<49:05,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 600: train loss {'ce_loss': 4.3406524658203125, 'f_loss': 4.291020393371582}, val loss {'ce_loss': 4.522536754608154, 'f_loss': 4.476134300231934}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 801/2000 [02:55<41:59,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 800: train loss {'ce_loss': 4.122776508331299, 'f_loss': 4.068578720092773}, val loss {'ce_loss': 4.346164703369141, 'f_loss': 4.296655178070068}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1001/2000 [03:38<34:59,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss {'ce_loss': 3.9153199195861816, 'f_loss': 3.854647159576416}, val loss {'ce_loss': 4.201709270477295, 'f_loss': 4.147448539733887}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1201/2000 [04:20<27:59,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1200: train loss {'ce_loss': 3.7825870513916016, 'f_loss': 3.7182061672210693}, val loss {'ce_loss': 4.100528717041016, 'f_loss': 4.043321132659912}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1401/2000 [05:02<20:58,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1400: train loss {'ce_loss': 3.6546132564544678, 'f_loss': 3.587021827697754}, val loss {'ce_loss': 4.020479202270508, 'f_loss': 3.9605300426483154}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1601/2000 [05:44<14:06,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1600: train loss {'ce_loss': 3.538849115371704, 'f_loss': 3.4687914848327637}, val loss {'ce_loss': 3.9473252296447754, 'f_loss': 3.8854753971099854}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1801/2000 [06:27<06:57,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1800: train loss {'ce_loss': 3.4556074142456055, 'f_loss': 3.3836452960968018}, val loss {'ce_loss': 3.8955559730529785, 'f_loss': 3.8320443630218506}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:02<00:00,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final training loss = 3.664591073989868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f_loss_list = []\n",
    "ce_loss_list = []\n",
    "for it in tqdm(range(LARGE_ITERS)):\n",
    "    # Evaluate\n",
    "    if it % eval_interval == 0:\n",
    "        losses = estimate_loss(tlm, EVAL_ITERS, CONTEXT_WINDOW_SIZE, device, use_focal_loss=True)\n",
    "        print(f\"step {it}: train loss {losses['train']}, val loss {losses['val']}\")\n",
    "    \n",
    "    # Forward/backward/update\n",
    "    xb, yb = get_batch('train', CONTEXT_WINDOW_SIZE, device, batch_size = 64)\n",
    "    logits, ce_loss, f_loss = tlm(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    f_loss.backward()\n",
    "    optimizer.step()\n",
    "    f_loss_list.append(f_loss.item())\n",
    "    ce_loss_list.append(ce_loss.item())\n",
    "\n",
    "print(\"final CE training loss =\", ce_loss_list[-1])\n",
    "print(\"final F training loss =\", f_loss_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'ce_loss' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m start_context = torch.zeros((\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m), dtype=torch.long, device=device)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m uncond_gen = (\u001b[43mtlm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONTEXT_WINDOW_SIZE\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m].tolist())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 313\u001b[39m, in \u001b[36mTransformerLM.generate\u001b[39m\u001b[34m(self, token_ids, max_new_tokens)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token_ids.size(\u001b[32m1\u001b[39m) > CONTEXT_WINDOW_SIZE:\n\u001b[32m    312\u001b[39m     token_ids = token_ids[:, -CONTEXT_WINDOW_SIZE:]\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m logits, _, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m logits = logits[:, -\u001b[32m1\u001b[39m, :]\n\u001b[32m    315\u001b[39m probs = F.softmax(logits, dim=-\u001b[32m1\u001b[39m) \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 299\u001b[39m, in \u001b[36mTransformerLM.forward\u001b[39m\u001b[34m(self, token_ids, targets)\u001b[39m\n\u001b[32m    297\u001b[39m     ce_loss = F.cross_entropy(logits.view(-\u001b[32m1\u001b[39m, logits.shape[-\u001b[32m1\u001b[39m]), targets.view(-\u001b[32m1\u001b[39m))\n\u001b[32m    298\u001b[39m     f_loss = focal_loss(logits.view(-\u001b[32m1\u001b[39m, logits.shape[-\u001b[32m1\u001b[39m]), targets.view(-\u001b[32m1\u001b[39m), gamma=\u001b[32m0.5\u001b[39m, alpha=\u001b[32m1.0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlogits\u001b[49m, ce_loss, f_loss\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'ce_loss' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "start_context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "uncond_gen = (tlm.generate(start_context, max_new_tokens=CONTEXT_WINDOW_SIZE)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old toow:--the whitive!\n",
      "Come, marrylee: say 'tis done.\n",
      "O, forswidder! I'll do back be dead\n",
      "HappfVignicious; a soldier and night!' Go\n",
      "What, of 'tis good\n",
      "Hapsont kingable\n",
      "Of last day, and soft. The king comes my sister\n",
      "'ANDam Pidupusion. Corthy boys, good Since\n",
      "Even Chiius, and S heartades daughter Wortiides my sad chawith\n",
      "Turnought with him: the mashood am told me\n",
      "The fatin of it. 'tis still wife, run in it and\n",
      "geloilt on what this pel'd, Hector, toward his witness:\n",
      "And they are half as two deeds willliarch!\n",
      "He hath already's raidings too: this swa times\n",
      "Some vain'd his own sound friend to fire;\n",
      "And, most does you other to me know.\n",
      "\n",
      "LUCENTIO:\n",
      "\n",
      "CASSIO:\n",
      "Kow your cel were she business.\n",
      "\n",
      "LUCENTIO:\n",
      "Let your other's sake to fear\n"
     ]
    }
   ],
   "source": [
    "print(decode_bpe(uncond_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch(split='train', context_window_size=CONTEXT_WINDOW_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"They mean to warn us at Philippi here,\\nAnswering before we do demand of them.\\n\\nANTONY:\\nTut, I am in their bosoms, and I know\\nWherefore they do it: they could be content\\nTo visit other places; and come down\\nWith fearful bravery, thinking by this face\\nTo fasten in our thoughts that they have courage;\\nBut 'tis not so.\\n\\nMessenger:\\nPrepare you, generals:\\nThe enemy comes on in gallant show;\\nTheir bloody sign of battle is hung out,\\nAnd something to be done immediately.\\n\\nANTONY:\\nOctavius, lead your battle softly on,\\nUpon the left hand of the even field.\\n\\nOCTAVIUS:\\nUpon the right hand I; keep thou the left.\\n\\nANTONY:\\nWhy do you cross me in this exigent?\\n\\nOCTAVIUS:\\nI do not cross you; but I will do so.\\n\\nBRUTUS:\\nThey stand, andiaught those strange su were indeed mother letter straight\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_bpe(tlm.generate(xb[0:1], max_new_tokens=10).tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
