{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "\n",
    "torch.manual_seed(305)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 4,573,338\n"
     ]
    }
   ],
   "source": [
    "input_file_path = 'full_shakespeare.txt'\n",
    "\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 67\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1257917 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 1,257,917 tokens\n",
      "val has 137,564 tokens\n"
     ]
    }
   ],
   "source": [
    "sorted_tokens = sorted(learned_tokens, key=lambda x: (len(x), x))\n",
    "stoi = {token: i for i, token in enumerate(sorted_tokens)}\n",
    "itos = {i: token for token, i in stoi.items()}\n",
    "\n",
    "def encode_bpe(text):\n",
    "    # Use the tokenizer's own tokenization method.\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [stoi[t] for t in tokens if t in stoi]\n",
    "\n",
    "def decode_bpe(indices):\n",
    "    return ''.join([itos[i] for i in indices])\n",
    "\n",
    "# Split the data into training and validation sets.\n",
    "n = len(data)\n",
    "train_text = data[:int(n * 0.9)]\n",
    "val_text = data[int(n * 0.9):]\n",
    "\n",
    "# Tokenize the text.\n",
    "train_tokens = encode_bpe(train_text)\n",
    "val_tokens = encode_bpe(val_text)\n",
    "\n",
    "train_data = torch.tensor(train_tokens)\n",
    "val_data = torch.tensor(val_tokens)\n",
    "\n",
    "print(f\"train has {len(train_data):,} tokens\")\n",
    "print(f\"val has {len(val_data):,} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting batches of data\n",
    "def get_batch(split, context_window_size, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    generate a small batch of data of inputs x and targets y\n",
    "\n",
    "    Args:\n",
    "        split: 'train' or 'val'\n",
    "        device: 'cpu' or 'cuda' (should be 'cuda' if available)\n",
    "    \"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_window_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_window_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_window_size+1] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# helper function for tracking loss during training\n",
    "# given to you\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters, context_window_size, device):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      model: model being evaluated\n",
    "      eval_iters: number of batches to average over\n",
    "      context_window_size: size of the context window\n",
    "      device: 'cpu' or 'cuda' (should be 'cuda' if available)\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, context_window_size, device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, context_window_size, embed_size=384):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          head_size: int, size of the head embedding dimension (K)\n",
    "          context_window_size: int, number of tokens considered in the past for attention (T)\n",
    "          embed_size: int, size of the token embedding dimension (D)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "        # not a param of the model, so registered as a buffer\n",
    "        self.register_buffer('tril', torch.tril(\n",
    "            torch.ones(context_window_size, context_window_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: (B,T,D) tensor of token embeddings\n",
    "\n",
    "        Returns:\n",
    "          (B,T,D) tensor of attention-weighted token embeddings\n",
    "        \"\"\"\n",
    "        # TODO: your code here\n",
    "        B, T, _ = x.shape\n",
    "        K = self.head_size\n",
    "        key = self.key(x)\n",
    "        query = self.query(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        attn_scores = query@key.transpose(-2, -1)\n",
    "        causal_mask = self.tril[:T, :T][None, :, :]\n",
    "        attn_scores = attn_scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "        attn_weights = torch.softmax(attn_scores / (K ** 0.5), dim=-1)\n",
    "        return attn_weights@value\n",
    "\n",
    "class SingleHeadedAttentionLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, context_window_size, head_size, embed_size=384):\n",
    "      \"\"\"\n",
    "      Args:\n",
    "        vocab_size: int, size of the vocabulary (V)\n",
    "        context_window_size: int, number of tokens considered in the past for attention (T)\n",
    "        head_size: int, size of the head embedding dimension (K)\n",
    "        embed_size: int, size of the token embedding dimension (D)\n",
    "      \"\"\"\n",
    "      super().__init__()\n",
    "      self.vocab_size = vocab_size\n",
    "      self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
    "      self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
    "      self.context_window_size = context_window_size\n",
    "\n",
    "      # TODO: your code below\n",
    "      self.atten_head = Head(head_size, context_window_size, embed_size)\n",
    "      self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, token_ids, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry\n",
    "                     in the batch has length T)\n",
    "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
    "\n",
    "        Returns:\n",
    "          logits: (B, T, V) logits[b,t] gives the length V vector of logits for the next token\n",
    "                   prediction in string b up to t tokens\n",
    "          loss: scalar, negative log likelihood of target given context\n",
    "        \"\"\"\n",
    "        B, T = token_ids.shape # (batch size, length)\n",
    "        tok_emb = self.token_embedding_table(token_ids) # (B,T,D)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,D)\n",
    "        x = tok_emb + pos_emb # (B,T,D)\n",
    "        x = self.atten_head(x) # (B,T,D)\n",
    "        logits = self.lm_head(x) # (B,T,V)\n",
    "\n",
    "        # TODO: your code here\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, token_ids, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          token_ids: (B, T) tensor of token ids to provide as context\n",
    "          max_new_tokens: int, maximum number of new tokens to generate\n",
    "\n",
    "        Returns:\n",
    "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "        # your code below\n",
    "        B, T = token_ids.shape\n",
    "        new_token_ids = token_ids.clone()\n",
    "        for t in range(max_new_tokens):\n",
    "            logits = self(new_token_ids)\n",
    "            new_token = torch.multinomial(F.softmax(logits[:, -1, :], dim=-1), 1)\n",
    "            new_token_ids = torch.cat([new_token_ids, new_token], dim=1)\n",
    "        return new_token_ids\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, context_window_size, num_heads, head_size, embed_size=384):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context_window_size: int, number of tokens considered in the past for attention (T)\n",
    "            num_heads: int, number of heads (H)\n",
    "            head_size: int, size of the head embedding dimension\n",
    "            embed_size: int, size of the token embedding dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO, your code below\n",
    "        self.heads = nn.ModuleList([Head(head_size, context_window_size, embed_size) for _ in range(num_heads)])\n",
    "        self.lm_head = nn.Linear(embed_size*num_heads, embed_size)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO, your code below\n",
    "        B, T, _ = x.shape\n",
    "        head_size = x.shape[-1] // self.num_heads\n",
    "        head_outputs = [head(x) for head in self.heads]\n",
    "        head_outputs = torch.cat(head_outputs, dim=-1)\n",
    "        head_outputs = head_outputs.view(B, T, -1)\n",
    "        return self.lm_head(head_outputs)\n",
    "\n",
    "class MultiHeadedAttentionLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6):\n",
    "      super().__init__()\n",
    "      self.head_size = embed_size // num_heads\n",
    "      self.context_window_size = context_window_size\n",
    "      # TODO: your code below\n",
    "      self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
    "      self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
    "      self.multi_head_attention = MultiHeadAttention(context_window_size, num_heads, self.head_size, embed_size)\n",
    "      self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "      self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, token_ids, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          token_ids: (B, T) token ids that make up the context (batch has size B, each entry in the\n",
    "                     batch has length T)\n",
    "          targets: (B, T) token ids corresponding to the target of each context in token_ids\n",
    "\n",
    "        Returns:\n",
    "          logits: (B, T, V), logits[b,t] gives the length V vector of logits for the next token\n",
    "                  prediction in string b up to t tokens\n",
    "          loss: scalar, negative log likelihood of target given context\n",
    "        \"\"\"\n",
    "        # TODO: your code below\n",
    "        loss = None\n",
    "        B, T = token_ids.shape\n",
    "        tok_emb = self.token_embedding_table(token_ids)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.multi_head_attention(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, token_ids, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          token_ids: (B, T) tensor of token ids to provide as context\n",
    "          max_new_tokens: int, maximum number of new tokens to generate\n",
    "\n",
    "        Returns:\n",
    "          (B, T+max_new_tokens) tensor of context with new tokens appended\n",
    "        \"\"\"\n",
    "        # TODO: your code below\n",
    "        for t in range(max_new_tokens):\n",
    "            if token_ids.shape[1] > self.context_window_size:\n",
    "                token_ids = token_ids[:, -self.context_window_size:]\n",
    "            B, T = token_ids.shape\n",
    "            logits, loss = self.forward(token_ids)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            new_token = torch.multinomial(probs, 1)\n",
    "            token_ids = torch.cat([token_ids, new_token], dim=1)\n",
    "        return token_ids\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity\n",
    "        Given to you, you don't need to write any code here!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\" Transformer block: communication across sequence length, followed by communication across embedding space\n",
    "        Uses multi-headed attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # TODO: your code below\n",
    "        self.feed_forward = FeedForward(embed_size)\n",
    "        self.atten_heads = MultiHeadAttention(context_window_size, num_heads, embed_size // num_heads, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.atten_heads(self.ln1(x)) # communication over sequence length\n",
    "        x = x + self.feed_forward(self.ln2(x)) # communication across embedding space\n",
    "        return x\n",
    "\n",
    "class TransformerLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, context_window_size, embed_size=384, num_heads=6, n_layers=6):\n",
    "        \"\"\"\n",
    "          Args:\n",
    "              vocab_size: int, number of tokens in the vocabulary (V)\n",
    "              context_window_size: int, size of the context window (T)\n",
    "              embed_size: int, embedding size (D)\n",
    "              num_heads: int, number of heads (H)\n",
    "              n_layers: int, number of layers (M)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding_table = nn.Embedding(context_window_size, embed_size)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(vocab_size,\n",
    "                             context_window_size,\n",
    "                             embed_size=embed_size,\n",
    "                             num_heads=num_heads)\n",
    "            for _ in range(n_layers)])\n",
    "\n",
    "        # final layer norm\n",
    "        self.ln_f = nn.LayerNorm(embed_size)\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "        # good initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, token_ids, targets=None):\n",
    "        \"\"\"\n",
    "        Agrgs:\n",
    "            token_ids: tensor of integers, provides the contet, shape (B, T)\n",
    "            targets: tensor of integers, provides the tokens we are preidcitng, shape (B, T)\n",
    "        \"\"\"\n",
    "        B, T = token_ids.shape\n",
    "\n",
    "        # token_ids and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(token_ids) # (B, T, D)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, D)\n",
    "        x = tok_emb + pos_emb # (B, T, D)\n",
    "\n",
    "        # TODO: your code below\n",
    "        loss = None\n",
    "        logits = self.blocks(x)\n",
    "        logits = self.ln_f(logits)\n",
    "        logits = self.lm_head(logits)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, token_ids, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: tensor of integers forming the context, shape (B, T)\n",
    "            max_new_tokens: int, max number of tokens to generate\n",
    "        \"\"\"\n",
    "        # TOOD, your code below\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            if token_ids.size(1) > CONTEXT_WINDOW_SIZE:\n",
    "                token_ids = token_ids[:, -CONTEXT_WINDOW_SIZE:]\n",
    "            logits, _ = self(token_ids)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "        self.train()\n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_ITERS = 1000\n",
    "LARGE_ITERS = 2000\n",
    "EVAL_ITERS = 100\n",
    "CONTEXT_WINDOW_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [4,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(LARGE_ITERS)):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m it % eval_interval == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         losses = \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEVAL_ITERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONTEXT_WINDOW_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Forward/backward/update\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mestimate_loss\u001b[39m\u001b[34m(model, eval_iters, context_window_size, device)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[32m     33\u001b[39m     X, Y = get_batch(split, context_window_size, device)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     logits, loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     losses[k] = loss.item()\n\u001b[32m     36\u001b[39m out[split] = losses.mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 275\u001b[39m, in \u001b[36mTransformerLM.forward\u001b[39m\u001b[34m(self, token_ids, targets)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;66;03m# token_ids and targets are both (B, T) tensor of integers\u001b[39;00m\n\u001b[32m    274\u001b[39m tok_emb = \u001b[38;5;28mself\u001b[39m.token_embedding_table(token_ids) \u001b[38;5;66;03m# (B, T, D)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m pos_emb = \u001b[38;5;28mself\u001b[39m.position_embedding_table(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# (T, D)\u001b[39;00m\n\u001b[32m    276\u001b[39m x = tok_emb + pos_emb \u001b[38;5;66;03m# (B, T, D)\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# TODO: your code below\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "trans = TransformerLM(vocab_size, CONTEXT_WINDOW_SIZE)\n",
    "tlm = trans.to(device)\n",
    "learning_rate = 1e-4\n",
    "# TODO, your code below\n",
    "\n",
    "optimizer = optim.Adam(tlm.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_list = []\n",
    "eval_interval = 200\n",
    "tlm.train()\n",
    "for it in tqdm(range(LARGE_ITERS)):\n",
    "    # Evaluate\n",
    "    if it % eval_interval == 0:\n",
    "        losses = estimate_loss(tlm, EVAL_ITERS, CONTEXT_WINDOW_SIZE, device)\n",
    "        print(f\"step {it}: train loss {losses['train']:.3f}, val loss {losses['val']:.3f}\")\n",
    "    \n",
    "    # Forward/backward/update\n",
    "    xb, yb = get_batch('train', CONTEXT_WINDOW_SIZE, device, batch_size = 32)\n",
    "    logits, loss = tlm(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "print(\"final training loss =\", loss_list[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_initial_vocab(text):\n",
    "    vocab = collections.Counter()\n",
    "    tokens = re.split(r'(\\s+)', text)\n",
    "    for token in tokens:\n",
    "        if token:\n",
    "            # Represent the token as a tuple of characters (preserving whitespace)\n",
    "            tokenized_token = tuple(token)\n",
    "            vocab[tokenized_token] += 1\n",
    "    return vocab\n",
    "\n",
    "def get_pair_stats(vocab):\n",
    "    pair_freqs = collections.Counter()\n",
    "    for tokenized_word, freq in vocab.items():\n",
    "        tokens = list(tokenized_word)\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pair = (tokens[i], tokens[i+1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    merged_token = \"\".join(pair)\n",
    "    new_vocab = {}\n",
    "    for tokenized_word, freq in vocab.items():\n",
    "        tokens = list(tokenized_word)\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n",
    "                new_tokens.append(merged_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        new_vocab[tuple(new_tokens)] = freq\n",
    "    return new_vocab\n",
    "\n",
    "def bpe_tokenizer(text, desired_vocab_size):\n",
    "    vocab = build_initial_vocab(text)\n",
    "    merges = []\n",
    "    \n",
    "    while True:\n",
    "        current_tokens = set()\n",
    "        for tokenized_word in vocab:\n",
    "            current_tokens.update(tokenized_word)  # tokenized_word is now a tuple\n",
    "        \n",
    "        if len(current_tokens) >= desired_vocab_size:\n",
    "            break\n",
    "        print(len(current_tokens))\n",
    "\n",
    "        \n",
    "        pair_stats = get_pair_stats(vocab)\n",
    "        if not pair_stats:\n",
    "            break\n",
    "        \n",
    "        best_pair = max(pair_stats, key=pair_stats.get)\n",
    "        merges.append(best_pair)\n",
    "        \n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "    \n",
    "    return merges, vocab, current_tokens\n",
    "\n",
    "def apply_bpe(word, merges):\n",
    "    # Start with a list of characters for the word\n",
    "    tokens = list(word)\n",
    "    for merge in merges:\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            if (tokens[i], tokens[i+1]) == merge:\n",
    "                tokens = tokens[:i] + [\"\".join(merge)] + tokens[i+2:]\n",
    "                i = max(i-1, 0)\n",
    "            else:\n",
    "                i += 1\n",
    "    return tokens\n",
    "\n",
    "def bpe_tokenize(text, merges):\n",
    "    # Split text preserving whitespace\n",
    "    pieces = re.split(r'(\\s+)', text)\n",
    "    pieces = [p for p in pieces if p]  # remove empty tokens\n",
    "    output = []\n",
    "    for piece in pieces:\n",
    "        # If piece is purely whitespace, keep it as is.\n",
    "        if piece.isspace():\n",
    "            output.append(piece)\n",
    "        else:\n",
    "            output.extend(apply_bpe(piece, merges))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_initial_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_tokens = set()\n",
    "for word in vocab:\n",
    "    current_tokens.update(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x2b2c58fbe7d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.local/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 1442\n",
      "68 1435\n",
      "69 1434\n",
      "70 1431\n",
      "71 1429\n",
      "72 1426\n",
      "73 1423\n",
      "74 1419\n",
      "75 1418\n",
      "76 1415\n",
      "77 1408\n",
      "78 1403\n",
      "79 1398\n"
     ]
    }
   ],
   "source": [
    "desired_vocab_size = 2000\n",
    "merges, final_vocab, learned_tokens = bpe_tokenizer(data, desired_vocab_size)\n",
    "\n",
    "sorted_tokens = sorted(learned_tokens, key=lambda x: (len(x), x))\n",
    "stoi = { token: i for i, token in enumerate(sorted_tokens) }\n",
    "itos = { i: token for token, i in stoi.items() }\n",
    "\n",
    "def encode_bpe(text):\n",
    "    tokens = bpe_tokenize(text, merges)\n",
    "    return [stoi[t] for t in tokens if t in stoi]\n",
    "\n",
    "def decode_bpe(indices):\n",
    "    return ''.join([itos[i] for i in indices])\n",
    "\n",
    "n = len(data)\n",
    "train_text = data[:int(n*0.9)]\n",
    "val_text = data[int(n*0.9):]\n",
    "\n",
    "train_tokens = encode_bpe(train_text)\n",
    "val_tokens = encode_bpe(val_text)\n",
    "\n",
    "train_data = torch.tensor(train_tokens)\n",
    "val_data = torch.tensor(val_tokens)\n",
    "\n",
    "print(f\"train has {len(train_data):,} tokens\")\n",
    "print(f\"val has {len(val_data):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
