{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "\n",
    "torch.manual_seed(305)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 4,573,338\n"
     ]
    }
   ],
   "source": [
    "input_file_path = 'full_shakespeare.txt'\n",
    "\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 67\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_initial_vocab(text):\n",
    "    vocab = collections.Counter()\n",
    "    tokens = re.split(r'(\\s+)', text)\n",
    "    for token in tokens:\n",
    "        if token:\n",
    "            # Represent the token as a tuple of characters (preserving whitespace)\n",
    "            tokenized_token = tuple(token)\n",
    "            vocab[tokenized_token] += 1\n",
    "    return vocab\n",
    "\n",
    "def get_pair_stats(vocab):\n",
    "    pair_freqs = collections.Counter()\n",
    "    for tokenized_word, freq in vocab.items():\n",
    "        tokens = list(tokenized_word)\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pair = (tokens[i], tokens[i+1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    merged_token = \"\".join(pair)\n",
    "    new_vocab = {}\n",
    "    for tokenized_word, freq in vocab.items():\n",
    "        tokens = list(tokenized_word)\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n",
    "                new_tokens.append(merged_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        new_vocab[tuple(new_tokens)] = freq\n",
    "    return new_vocab\n",
    "\n",
    "def bpe_tokenizer(text, desired_vocab_size):\n",
    "    vocab = build_initial_vocab(text)\n",
    "    merges = []\n",
    "    \n",
    "    while True:\n",
    "        current_tokens = set()\n",
    "        for tokenized_word in vocab:\n",
    "            current_tokens.update(tokenized_word)  # tokenized_word is now a tuple\n",
    "        \n",
    "        if len(current_tokens) >= desired_vocab_size:\n",
    "            print(len(current_tokens))\n",
    "            break\n",
    "        \n",
    "        pair_stats = get_pair_stats(vocab)\n",
    "        if not pair_stats:\n",
    "            break\n",
    "        \n",
    "        best_pair = max(pair_stats, key=pair_stats.get)\n",
    "        merges.append(best_pair)\n",
    "        \n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "    \n",
    "    return merges, vocab, current_tokens\n",
    "\n",
    "def apply_bpe(word, merges):\n",
    "    # Start with a list of characters for the word\n",
    "    tokens = list(word)\n",
    "    for merge in merges:\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            if (tokens[i], tokens[i+1]) == merge:\n",
    "                tokens = tokens[:i] + [\"\".join(merge)] + tokens[i+2:]\n",
    "                i = max(i-1, 0)\n",
    "            else:\n",
    "                i += 1\n",
    "    return tokens\n",
    "\n",
    "def bpe_tokenize(text, merges):\n",
    "    # Split text preserving whitespace\n",
    "    pieces = re.split(r'(\\s+)', text)\n",
    "    pieces = [p for p in pieces if p]  # remove empty tokens\n",
    "    output = []\n",
    "    for piece in pieces:\n",
    "        # If piece is purely whitespace, keep it as is.\n",
    "        if piece.isspace():\n",
    "            output.append(piece)\n",
    "        else:\n",
    "            output.extend(apply_bpe(piece, merges))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_vocab_size = 2000\n",
    "merges, final_vocab, learned_tokens = bpe_tokenizer(data, desired_vocab_size)\n",
    "\n",
    "sorted_tokens = sorted(learned_tokens, key=lambda x: (len(x), x))\n",
    "stoi = { token: i for i, token in enumerate(sorted_tokens) }\n",
    "itos = { i: token for token, i in stoi.items() }\n",
    "\n",
    "def encode_bpe(text):\n",
    "    tokens = bpe_tokenize(text, merges)\n",
    "    return [stoi[t] for t in tokens if t in stoi]\n",
    "\n",
    "def decode_bpe(indices):\n",
    "    return ''.join([itos[i] for i in indices])\n",
    "\n",
    "n = len(data)\n",
    "train_text = data[:int(n*0.9)]\n",
    "val_text = data[int(n*0.9):]\n",
    "\n",
    "train_tokens = encode_bpe(train_text)\n",
    "val_tokens = encode_bpe(val_text)\n",
    "\n",
    "train_data = torch.tensor(train_tokens)\n",
    "val_data = torch.tensor(val_tokens)\n",
    "\n",
    "print(f\"train has {len(train_data):,} tokens\")\n",
    "print(f\"val has {len(val_data):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
